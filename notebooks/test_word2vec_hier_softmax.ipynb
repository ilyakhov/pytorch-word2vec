{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import logging\n",
    "\n",
    "from process_data import process_data\n",
    "from model_fn import CBOWHierSoftmax\n",
    "from input_fn import CBOWBibleDataset\n",
    "from utils import set_logger\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('embeddings.weight', tensor([[-0.0799, -0.0462,  0.0638,  ...,  0.0286, -0.0208,  0.0190],\n",
      "        [-0.0036, -0.0701, -0.0067,  ...,  0.0657,  0.0238, -0.0051],\n",
      "        [ 0.0782,  0.0749, -0.0496,  ..., -0.0327, -0.1115, -0.0365],\n",
      "        ...,\n",
      "        [-0.0634, -0.0418, -0.0063,  ...,  0.0294, -0.0104,  0.0099],\n",
      "        [-0.0558, -0.1040, -0.0373,  ..., -0.0392, -0.0563,  0.0774],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0'))])\n",
      "tensor([-7.9949e-02, -4.6223e-02,  6.3770e-02,  2.4370e-02, -3.1488e-02,\n",
      "        -1.9627e-02, -1.3042e-01, -2.3389e-02,  8.9069e-02, -3.5825e-02,\n",
      "        -7.1607e-02, -1.2580e-02, -6.0275e-02,  8.1855e-03, -4.0679e-02,\n",
      "        -1.4761e-01, -3.0337e-02, -3.0497e-02, -3.9316e-02, -6.6072e-02,\n",
      "        -1.4983e-01, -7.6077e-02, -6.2963e-02, -1.8337e-02, -3.2213e-03,\n",
      "        -1.6931e-01, -7.7741e-03,  5.0829e-02,  1.2319e-02, -7.4092e-02,\n",
      "        -9.7028e-02,  1.6195e-02,  7.9325e-02, -1.1648e-01,  1.3251e-02,\n",
      "        -1.1220e-03, -3.6863e-02, -5.1580e-03, -4.3979e-02,  3.7527e-02,\n",
      "        -1.4069e-01,  6.8157e-02,  3.0229e-02,  1.7723e-02,  2.2076e-02,\n",
      "         2.0536e-03, -9.3967e-02, -2.5638e-03, -7.3687e-02, -1.7435e-02,\n",
      "        -2.6477e-02,  1.0577e-01,  5.9184e-02,  1.5834e-02, -3.2331e-02,\n",
      "         1.1614e-01,  5.5210e-02, -5.6091e-02,  8.6053e-02, -2.5750e-02,\n",
      "         5.0490e-02,  5.4594e-03,  1.6504e-01,  1.3031e-01,  1.0764e-01,\n",
      "        -9.1258e-02, -5.2935e-02,  1.4809e-01,  1.2188e-01,  2.4888e-02,\n",
      "        -1.1566e-01, -1.0372e-01, -3.1288e-02,  4.6014e-02, -1.9411e-02,\n",
      "         3.6762e-02, -9.3757e-02, -6.7080e-02, -5.1009e-02,  1.6981e-02,\n",
      "         2.4217e-02,  5.6495e-02, -3.3958e-02,  4.6245e-03, -1.3270e-02,\n",
      "        -3.1153e-02, -6.5855e-02,  1.7314e-01,  1.3088e-02, -1.0712e-01,\n",
      "         1.8518e-02,  8.5687e-02,  3.7489e-02,  3.8863e-02, -8.3267e-02,\n",
      "         1.5211e-03, -3.2099e-02, -4.3603e-02, -6.2967e-02, -5.7304e-03,\n",
      "        -8.4542e-03, -1.3065e-01,  4.8996e-03, -6.2431e-02, -9.2914e-02,\n",
      "         3.5887e-03,  4.6292e-03, -4.6452e-02, -7.7353e-02,  1.4417e-01,\n",
      "        -9.2596e-03,  8.6630e-02, -2.2214e-03,  4.1035e-03, -1.2132e-01,\n",
      "         1.6785e-02,  8.4446e-03,  3.1834e-02,  9.5864e-02, -7.0619e-02,\n",
      "         6.0701e-02,  1.8444e-02, -2.6416e-02, -1.5255e-01, -8.9766e-03,\n",
      "        -3.3733e-02,  1.2730e-01,  1.1001e-01,  5.5367e-02,  2.4502e-02,\n",
      "         3.0934e-02,  5.2876e-02,  3.3605e-02,  1.2634e-01, -8.3189e-03,\n",
      "         1.2054e-01,  5.2608e-02, -1.5484e-02,  2.6365e-02, -6.5610e-03,\n",
      "         9.6002e-02, -1.2106e-01,  3.3763e-02, -2.8438e-02, -5.1442e-02,\n",
      "         2.7438e-02,  5.6398e-02,  5.7892e-02,  7.1970e-03, -1.5757e-01,\n",
      "         1.5979e-03, -2.3189e-02, -5.1996e-02,  7.7979e-02, -5.7309e-02,\n",
      "         4.8337e-02,  5.7907e-02, -4.0557e-02,  1.3849e-02, -8.1730e-02,\n",
      "         5.7527e-02, -5.6232e-02,  1.1124e-01, -3.7576e-02,  1.2056e-01,\n",
      "        -2.0266e-02,  1.9980e-02,  3.2968e-02, -2.2633e-02, -7.2351e-02,\n",
      "         8.9399e-02,  1.3018e-01, -4.2021e-02,  6.7472e-02, -1.0129e-01,\n",
      "         5.3186e-02, -8.9783e-02,  1.5733e-02,  5.2951e-02, -6.3529e-02,\n",
      "        -4.2734e-02,  1.3944e-01,  2.0217e-02, -5.0483e-02,  1.0495e-01,\n",
      "        -1.9400e-02,  1.2745e-01, -7.6274e-05,  9.9331e-02,  4.6550e-02,\n",
      "         1.0068e-01, -7.4808e-03, -1.1603e-01,  7.6355e-02, -7.5499e-02,\n",
      "        -7.3423e-02, -8.9219e-02,  2.8648e-02, -2.0815e-02,  1.8960e-02],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "# WARNING! trashy code inside: skip it to the next cells\n",
    "\n",
    "try:\n",
    "    huffman_corpus, node_inxs, turns_inxs, leaves_hash_inversed, \\\n",
    "        vocab_size, extended_vocab_size = \\\n",
    "        pickle.load(open('/tmp/bible_dataset.pkl', 'rb'))\n",
    "except FileNotFoundError:\n",
    "    out = process_data('../', 5)\n",
    "    # print(out)\n",
    "    pickle.dump(out, open('/tmp/bible_dataset.pkl', 'wb'))\n",
    "    huffman_corpus, node_inxs, turns_inxs, leaves_hash_inversed, \\\n",
    "        vocab_size, extended_vocab_size = out\n",
    "\n",
    "nodes_count = extended_vocab_size\n",
    "\n",
    "# with torch.cuda.device(device):\n",
    "# with torch.\n",
    "device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:0\")\n",
    "# device = None\n",
    "\n",
    "batch_size = 128  # 1024*8\n",
    "log_freq   = 100*8*2\n",
    "lr = 0.1\n",
    "\n",
    "\n",
    "st = time.time()\n",
    "# with torch.cuda.device(device):\n",
    "cbow_dataset = CBOWBibleDataset(huffman_corpus, node_inxs, turns_inxs,\n",
    "                                vocab_size=nodes_count,\n",
    "                                window_size=10,\n",
    "                                device=None)\n",
    "data_len = cbow_dataset.__len__()\n",
    "n_steps  = (data_len - 1) // batch_size\n",
    "cbow_loader = DataLoader(cbow_dataset, batch_size=batch_size,\n",
    "                         shuffle=False, num_workers=12)\n",
    "\n",
    "# loss = torch.mean(-1 * torch.log(cbow_out))\n",
    "losses = []\n",
    "model = CBOWHierSoftmax(nodes_count, 200)\n",
    "model.cuda(0)\n",
    "\n",
    "\n",
    "path = '/home/d3/study-projects/really_new/doc2vec/pytorch-word2vec/ckpt-lambda-scheduler/e199-lr0.001-loss5.236-w2vec-bible.ckpt.tar'\n",
    "path1 = 'ckpt/e199-lr0.004-loss5.187-w2vec-bible.ckpt.tar'\n",
    "\n",
    "torch_loaded = torch.load(path1)\n",
    "    \n",
    "print(torch_loaded['model_state_dict'])\n",
    "model.load_state_dict(torch_loaded['model_state_dict'])\n",
    "# model.load_state_dict(torch_loaded['model_state_dict']['embeddings.weight'])\n",
    "print(model.embeddings.weight[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaves_hash = {v:k for k, v in leaves_hash_inversed.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5023"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaves_hash['jesus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_weights = model.embeddings.weight.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10670, 200)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5335, 200)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec = embedding_weights[:vocab_size]\n",
    "word2vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embeddings_index_of_word(word, leaves_hash=leaves_hash):\n",
    "    huffman_inx = leaves_hash[word]\n",
    "    return huffman_inx\n",
    "\n",
    "def get_embedding_by_word(word, word2vec=word2vec):\n",
    "    return word2vec[get_embeddings_index_of_word(word)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.55 s, sys: 72 Âµs, total: 7.55 s\n",
      "Wall time: 7.55 s\n"
     ]
    }
   ],
   "source": [
    "nbrs = NearestNeighbors(n_neighbors=25, algorithm='auto',\n",
    "                        metric='minkowski', p=2).fit(word2vec)\n",
    "%time distances, indices = nbrs.kneighbors(word2vec)\n",
    "# distances, indices = nbrs.kneighbors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_nn_neighbours(w, indices=indices, distances=distances):\n",
    "    print(f'word: {w}\\nneighbours: ')\n",
    "    inx = get_embeddings_index_of_word(w)\n",
    "    for w_inx, dist in zip(indices[inx], distances[inx]):\n",
    "        print('{} - {}'.format(get_word_by_embedding_index(w_inx), round(dist, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: jesus\n",
      "neighbours: \n",
      "jesus - 0.0\n",
      "pilate - 0.82\n",
      "peter - 0.84\n",
      "elias - 0.86\n",
      "job - 0.86\n",
      "elisha - 0.87\n",
      "caesar - 0.88\n",
      "samson - 0.89\n",
      "elijah - 0.89\n",
      "nazareth - 0.9\n",
      "baptized - 0.9\n",
      "others - 0.91\n",
      "herod - 0.91\n",
      "crucified - 0.91\n",
      "manoah - 0.91\n",
      "thank - 0.91\n",
      "already - 0.91\n",
      "answering - 0.91\n",
      "past - 0.91\n",
      "disciple - 0.91\n",
      "eli - 0.92\n",
      "parable - 0.92\n",
      "philip - 0.92\n",
      "baptism - 0.92\n",
      "barabbas - 0.92\n"
     ]
    }
   ],
   "source": [
    "print_nn_neighbours('jesus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cos sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cos_sim_matrix = cosine_similarity(word2vec, word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word_by_embedding_index(inx, leaves_hash_inversed=leaves_hash_inversed):\n",
    "    word = leaves_hash_inversed[inx]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_neighbours_cosine(w, cos_sim_matrix=cos_sim_matrix, topn=10):\n",
    "    print(f'word: {w}\\nneighbours: ')\n",
    "    inx = get_embeddings_index_of_word(w)\n",
    "        \n",
    "    word_row_dists = cos_sim_matrix[inx]\n",
    "    neighbours = np.argsort(-1 * word_row_dists)[:topn]\n",
    "    for n in neighbours:\n",
    "        print('{} - {:<3}'.format(get_word_by_embedding_index(n), round(word_row_dists[n], 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: jesus\n",
      "neighbours: \n",
      "jesus - 1.0\n",
      "peter - 0.6499999761581421\n",
      "pilate - 0.5799999833106995\n",
      "christ - 0.5600000023841858\n",
      "paul - 0.5400000214576721\n",
      "certain - 0.5400000214576721\n",
      "john - 0.5400000214576721\n",
      "elias - 0.5199999809265137\n",
      "job - 0.5\n",
      "nathanael - 0.5\n",
      "lazarus - 0.5\n",
      "elisha - 0.49000000953674316\n",
      "barabbas - 0.49000000953674316\n",
      "believed - 0.49000000953674316\n",
      "privately - 0.47999998927116394\n",
      "caesar - 0.47999998927116394\n",
      "elijah - 0.4699999988079071\n",
      "grace - 0.4699999988079071\n",
      "samson - 0.46000000834465027\n",
      "abraham - 0.46000000834465027\n",
      "question - 0.46000000834465027\n",
      "baptized - 0.44999998807907104\n",
      "nazareth - 0.44999998807907104\n",
      "festus - 0.44999998807907104\n",
      "manoah - 0.44999998807907104\n"
     ]
    }
   ],
   "source": [
    "print_neighbours_cosine('jesus', topn=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: god\n",
      "neighbours: \n",
      "god - 1.0\n",
      "salvation - 0.6399999856948853\n",
      "hosts - 0.6200000047683716\n",
      "hope - 0.6100000143051147\n",
      "grace - 0.6000000238418579\n",
      "word - 0.5699999928474426\n",
      "goodness - 0.5699999928474426\n",
      "glory - 0.5600000023841858\n",
      "righteousness - 0.5299999713897705\n",
      "fear - 0.5199999809265137\n",
      "christ - 0.5199999809265137\n",
      "truth - 0.5099999904632568\n",
      "prayer - 0.5099999904632568\n",
      "care - 0.49000000953674316\n",
      "lovingkindness - 0.47999998927116394\n",
      "lord - 0.47999998927116394\n",
      "chosen - 0.47999998927116394\n",
      "chastisement - 0.47999998927116394\n",
      "covenant - 0.47999998927116394\n",
      "faith - 0.4699999988079071\n",
      "enquire - 0.4699999988079071\n",
      "request - 0.4699999988079071\n",
      "power - 0.4699999988079071\n",
      "am - 0.46000000834465027\n",
      "delight - 0.46000000834465027\n"
     ]
    }
   ],
   "source": [
    "print_neighbours_cosine('god', topn=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec.Word2Vec?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
